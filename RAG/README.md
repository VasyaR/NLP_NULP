DISCLAIMER!!! THIS PROJECT IS DEDICATED ONLY FOR EDUCATIONAL AND ENTERTAINMENT PURPOSE AND NOT TO OFFEND OR HUMILIATE SOMEONE!

This project is dedicated for the creation of black humor generating bot. It is not only using the concept of RAG but 
is also dedicated to prompt hacking exploring. The helping context for model prompt is generated by files with offensive speech to make LLM less constricted, twitter and reddit jokes. All data files have ending _preprocessed.json. 
Data preprocessing can be seen in data_playground.py. 
app.py is responsible for using LLM using gradio.
Retriever.py is used for getting 'n' docs by similiraties using BM25 and/or semantic from embeddings using 'sentence-transformers/all-distilroberta-v1'. 
Semantic embeddings are stored partically to avoid RAM overusing. However even this can be not enough for process to avoid state where he is being killed due to RAM shortage so maybe you will need to use swap memory. Also the logic of getting doc embeddings should be reworked in retriever.py with the use of device.cuda. I have created embeddings in kaggle kernels with separate from project create-embeddings-for-semantic-score.ipynb. 
For tokenization I used lowercasing + NLTK library(It's usage is in tokenizing.py file). 
In LLM_usage.py it is described the logic of retrieving responces from 'groq/llama3-8b-8192'. 
I have considered to not use reranker because of lack of time mostly :( I will probably implement it later 

Using instructions