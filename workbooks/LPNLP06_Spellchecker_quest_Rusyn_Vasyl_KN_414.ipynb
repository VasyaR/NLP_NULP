{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spellchecker quest\n",
        "\n",
        "–•—Ç–æ—Å—å –Ω–∞—Ä–æ–±–∏–≤ –ø–æ–º–∏–ª–æ–∫ —É –≤—ñ—Ä—à–∞—Ö –¢–∞—Ä–∞—Å–∞ –®–µ–≤—á–µ–Ω–∫–∞. –ù–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ —Ü—ñ –ø–æ–º–∏–ª–∫–∏ —ñ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "\n",
        "## –ó–∞–¥–∞—á–∞\n",
        "\n",
        "–í–∏ –æ—Ç—Ä–∏–º–∞—î—Ç–µ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ç–∞ —Ç–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
        "\n",
        "–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.train_text`. –¶–µ –∑–≤–∏—á–∞–π–Ω–∏–π –Ω–µ—Ä–æ–∑–º—ñ—á–µ–Ω–∏–π —Ç–µ–∫—Å—Ç. –ù–∞ –Ω—å–æ–º—É –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –Ω–∞—Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å. –ü—ñ–¥—ñ–π–¥–µ –±—É–¥—å-—è–∫–∞. –Ø –±–∏ —Ä–∞–¥–∏–≤ feed-forward –Ω–µ–π—Ä–æ–Ω—É –º–æ–¥–µ–ª—å –∑ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—î—é –ø–æ –ª—ñ—Ç–µ—Ä–∞—Ö, –±–æ —Ü–µ —Ç–µ, —â–æ –º–∏ –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π –ª–µ–∫—Ü—ñ—ó. –ê–ª–µ n-–≥—Ä–∞–º–Ω–∞ —Ç–µ–∂ –º–∞—î —Å–ø—Ä–∞—Ü—é–≤–∞—Ç–∏.\n",
        "\n",
        "–¢–µ—Å—Ç—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –ø–æ–ª—ñ `lab.test_items`. –ü—Ä–∏–∫–ª–∞–¥ –æ–¥–Ω–æ–≥–æ –µ–ª–µ–º–µ–Ω—Ç–∞:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"text\": \"–°–ø—ñ–≤–∞–ª–∏ –± –ø—Ä–æ–∑—É, —Ç–∞ –ø–æ –Ω–æ–∂–∞—Ö,\",\n",
        "  \"error_start\": 23,\n",
        "  \"error_end\": 28,\n",
        "  \"error\": \"–Ω–æ–∂–∞—Ö\",\n",
        "  \"corrections\": [\n",
        "    \"–Ω–æ–≥–∞—Ö\",\n",
        "    \"–π–æ—Ç–∞—Ö\",\n",
        "    \"—î–Ω–æ—Ç–∞—Ö\",\n",
        "    \"–Ω–æ–∂–∞—Ö\",\n",
        "    \"–Ω–æ—Ç–∞—Ö\"\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "`error_start` —Ç–∞ `error_end` –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ –º—ñ—Å—Ü–µ–∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –ø–æ–º–∏–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç—ñ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö). –£ –¥–∞–Ω–Ω–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ, –ø–æ–º–∏–ª–∫–æ—é —î `text[23:28]`, —Ç–æ–±—Ç–æ —Å–ª–æ–≤–æ \"–Ω–æ–∂–∞—Ö\".\n",
        "\n",
        "`corrections` -- —Ü–µ —Å–ø–∏—Å–æ–∫ –º–æ–∂–ª–∏–≤–∏—Ö –≤–∏–ø—Ä–∞–≤–ª–µ–Ω—å.\n",
        "\n",
        "–í–∞—à–∞ –∑–∞–¥–∞—á–∞ -- –æ–±—Ä–∞—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —Å–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ–ø–æ–Ω–æ–≤–∞–Ω–∏—Ö.\n",
        "\n",
        "\n",
        "## –ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "\n",
        "–û–¥–∏–Ω –ø—Ä–∏–∫–ª–∞–¥ –≤ `lab.test_items` –¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ –æ–¥–Ω—É –ª—ñ—Ç–µ—Ä—É –ø—Ä–∏—Ö–æ–≤–∞–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è. –î–ª—è —Ü—å–æ–≥–æ –∑–Ω–∞–π–¥—ñ—Ç—å —Ä—ñ–∑–Ω–∏—Ü—é –º—ñ–∂ –ª—ñ—Ç–µ—Ä–∞–º–∏ —Å–ª–æ–≤–∞ –∑ –ø–æ–º–∏–ª–∫–æ—é (`error`) —Ç–∞ –æ–±—Ä–∞–Ω–∏–º –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º. –ù–∞–¥—Ä—É–∫—É–π—Ç–µ —Ü—é –ª—ñ—Ç–µ—Ä—É. –Ø–∫—â–æ —Å–ª–æ–≤–æ –∑ –ø–æ–º–∏–ª–∫–æ—é –Ω–∞–ø—Ä–∞–≤–¥—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–µ, –∞ —Ç–∞–∫–µ —Ç–µ–∂ –±—É–≤–∞—î, –Ω–∞–¥—Ä—É–∫—É–π—Ç–µ –ø—Ä–æ–±—ñ–ª. –ü—Ä–∏–∫–ª–∞–¥–∏:\n",
        "\n",
        "```\n",
        "Error               Correction     To print\n",
        "-------------------------------------------\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–ª—ñ—Ç        –ª\n",
        "–ø–Ω—ñ                  –ø–æ–Ω—ñ          –æ\n",
        "–±–∞–ª–ª–µ—Ç               –±–∞–ª–µ—Ç         –ª\n",
        "–ø—Ä–∏–≤—ñ—Ç               –ø—Ä–∏–≤—ñ—Ç        (space)\n",
        "```\n",
        "\n",
        "–ü—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —è–∫–µ –≤–∏ –ø–æ–±–∞—á–∏—Ç–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ —Ü–µ —Ä—è–¥–æ–∫ –∑ –≤—ñ—Ä—à—É –æ–¥–Ω–æ–≥–æ –∑ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –∞–≤—Ç–æ—Ä—ñ–≤.\n",
        "\n",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –∫–≤–µ—Å—Ç -- —ñ–º'—è –∞–≤—Ç–æ—Ä–∞/–∫–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ \"–Ü–º'—è –ü—Ä—ñ–∑–≤–∏—â–µ\".\n",
        "\n",
        "–ü–æ–ª–µ—Ç—ñ–ª–∏! üöÄ"
      ],
      "metadata": {
        "id": "L-Z5-FrvSNPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --ignore-installed http://nlp.band/static/pypy/lpnlp-2023.10.2-py3-none-any.whl"
      ],
      "metadata": {
        "id": "qa35VG1zj2wR",
        "outputId": "0600c621-7b7d-4baf-ab56-dad5c107f862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/144.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m143.4/144.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lpnlp\n",
        "\n",
        "lab = lpnlp.start(\n",
        "    email=\"vasyl.rusyn.kn.2021@lpnu.ua\",                   # <----------- –ó–∞–ø–æ–≤–Ω—ñ—Ç—å —Ü–µ –ø–æ–ª–µ\n",
        "    lab=\"quest_spellchecker\"\n",
        "    )"
      ],
      "metadata": {
        "id": "DjClbL-Hmcaq",
        "outputId": "f44f53f2-e4f7-44d2-ed4d-d9c33e41c2af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–£–¥–∞—á—ñ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ú–æ–≤–Ω–∞ –º–æ–¥–µ–ª—å\n",
        "\n",
        "–ù–∞—Ç—Ä–µ–Ω—É–π—Ç–µ —Å–≤–æ—é –º–æ–≤–Ω—É –º–æ–¥–µ–ª—å —Ç—É—Ç"
      ],
      "metadata": {
        "id": "CkQ69irsm_Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "oRS-i4J9FGSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Iterable"
      ],
      "metadata": {
        "id": "mjcvXigqFHg5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classes"
      ],
      "metadata": {
        "id": "BWEJ1hebE4Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "\n",
        "  def __init__(self, tokens, unk_token=\"<unk>\"):\n",
        "    self.unk_token = unk_token\n",
        "    self.unk_index = 0\n",
        "    self._itos = set([unk_token] + tokens)\n",
        "    self._stoi = {token: index for index, token in enumerate(self._itos)}\n",
        "\n",
        "  def stoi(self, token: str) -> int:\n",
        "    \"\"\"Return token index or `<unk>` index if `token` is not in the vocab.\n",
        "    \"\"\"\n",
        "    return self._stoi.get(token, self.unk_index)\n",
        "\n",
        "\n",
        "  def itos(self, index: int) -> str:\n",
        "    \"\"\"Return token by its `index`.\n",
        "\n",
        "    Raise LookupError if `index` is out of vocabulary range.\n",
        "    \"\"\"\n",
        "\n",
        "    return self._itos[index]\n",
        "\n",
        "  @property\n",
        "  def tokens(self):\n",
        "    return self._itos\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self._itos)\n",
        "\n",
        "class BengioLMModel(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embed_dim: int, context_len: int, hidden_dim: int) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.context_len = context_len\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim) # vocab_size * embed_dim\n",
        "        self.W = nn.Linear(context_len * embed_dim, hidden_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.U = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, X_indexes: torch.tensor):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            X_indexes: tensor of indexes of context tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        X = self.embed(X_indexes) # [batch_size, context len * embed dim]\n",
        "        e = X.view(-1, self.context_len * self.embed_dim)\n",
        "\n",
        "        h = self.tanh(self.W(e))\n",
        "\n",
        "        logits = self.U(h)\n",
        "\n",
        "        log_probs = torch.log_softmax(logits, dim=-1)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "Au9dQQYWE5cz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helping functions"
      ],
      "metadata": {
        "id": "u8gYVPgpE7Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> [str]:\n",
        "   return list(text.lower())\n",
        "\n",
        "def prepare_data(tokens: [str], context_len: int) -> [([str], str)]:\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        tokens: list of tokens\n",
        "        context_len: length of context\n",
        "\n",
        "    Reurns:\n",
        "        Iterable of (context, target) pairs\n",
        "    \"\"\"\n",
        "    # res = []\n",
        "\n",
        "    for i in range(context_len, len(tokens)):\n",
        "        context = tokens[i - context_len:i]\n",
        "        target = tokens[i]\n",
        "\n",
        "        yield (context, target)\n",
        "\n",
        "def batch_it(xs, batch_size):\n",
        "    batch = []\n",
        "\n",
        "    for i, x in enumerate(xs):\n",
        "        batch.append(x)\n",
        "\n",
        "        if i % batch_size == batch_size - 1:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "    if batch:\n",
        "        yield batch\n",
        "\n",
        "def vectorize(tokens: Iterable[str], vocab: Vocabulary) -> torch.tensor:\n",
        "   X = torch.tensor([vocab.stoi(token) for token in tokens])\n",
        "   return X"
      ],
      "metadata": {
        "id": "HGBDrjUdE9WZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lab.train_text[:330])"
      ],
      "metadata": {
        "id": "DsyqC9fPnMGa",
        "outputId": "24fef599-bbe3-4146-e034-e009e4774e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ôªø–ü–†–ò–ß–ò–ù–ù–ê\r\n",
            "\r\n",
            "–†–µ–≤–µ —Ç–∞ —Å—Ç–æ–≥–Ω–µ –î–Ω—ñ–ø—Ä —à–∏—Ä–æ–∫–∏–π,\r\n",
            "–°–µ—Ä–¥–∏—Ç–∏–π –≤—ñ—Ç–µ—Ä –∑–∞–≤–∏–≤–∞,\r\n",
            "–î–æ–¥–æ–ª—É –≤–µ—Ä–±–∏ –≥–Ω–µ –≤–∏—Å–æ–∫—ñ,\r\n",
            "–ì–æ—Ä–∞–º–∏ —Ö–≤–∏–ª—é –ø—ñ–¥—ñ–π–º–∞.\r\n",
            "–Ü –±–ª—ñ–¥–∏–π –º—ñ—Å—è—Ü—å –Ω–∞ —Ç—É –ø–æ—Ä—É\r\n",
            "–Ü–∑ —Ö–º–∞—Ä–∏ –¥–µ-–¥–µ –≤–∏–≥–ª—è–¥–∞–≤,\r\n",
            "–ù–µ–Ω–∞—á–µ —á–æ–≤–µ–Ω –≤ —Å–∏–Ω—ñ–º –º–æ—Ä—ñ,\r\n",
            "–¢–æ –≤–∏—Ä–∏–Ω–∞–≤, —Ç–æ –ø–æ—Ç–æ–ø–∞–≤.\r\n",
            "–©–µ —Ç—Ä–µ—Ç—ñ –ø—ñ–≤–Ω—ñ –Ω–µ —Å–ø—ñ–≤–∞–ª–∏,\r\n",
            "–ù—ñ—Ö—Ç–æ –Ω—ñ–≥–¥–µ –Ω–µ –≥–æ–º–æ–Ω—ñ–≤,\r\n",
            "–°–∏—á—ñ –≤ –≥–∞—é –ø–µ—Ä–µ–∫–ª–∏–∫–∞–ª–∏—Å—å,\r\n",
            "–¢–∞ —è—Å–µ–Ω —Ä–∞–∑ —É —Ä–∞–∑ —Å–∫—Ä–∏–ø—ñ–≤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "\n",
        "##### –í–∞—à –∫–æ–¥ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ —Ç—É—Ç\n",
        "\n",
        "train_text_tokens = tokenize(lab.train_text)\n",
        "vocab = Vocabulary(train_text_tokens)\n",
        "print(len(vocab))\n",
        "\n",
        "hparams = {\n",
        "        \"vocab_size\": len(vocab),\n",
        "        \"embed_dim\": 128,\n",
        "        \"context_len\": 15,\n",
        "        \"hidden_dim\": 256,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"num_epochs\": 10,\n",
        "        \"batch_size\": 4096\n",
        "    }\n",
        "\n",
        "model = BengioLMModel(vocab_size=hparams[\"vocab_size\"], embed_dim=hparams[\"embed_dim\"],\n",
        "                          context_len=hparams[\"context_len\"], hidden_dim=hparams[\"hidden_dim\"])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"learning_rate\"])\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "best_loss = 999999\n",
        "eraly_stop = 0\n",
        "\n",
        "for epoch in range(hparams[\"num_epochs\"]):\n",
        "\n",
        "  total_loss = 0.0\n",
        "  examples = prepare_data(train_text_tokens, hparams[\"context_len\"])\n",
        "  examples = list(examples)\n",
        "\n",
        "  for batch in tqdm(batch_it(examples, batch_size=hparams[\"batch_size\"]), leave=False):\n",
        "\n",
        "    X_batch = []\n",
        "    y_batch = []\n",
        "\n",
        "    for context, target in batch:\n",
        "      X = vectorize(context, vocab)\n",
        "      y = vectorize([target], vocab)\n",
        "      X_batch.append(X)\n",
        "      y_batch.append(y)\n",
        "\n",
        "    X_batch = torch.stack(X_batch)\n",
        "    y_batch = torch.tensor(y_batch)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    log_probs = model(X_batch)\n",
        "    loss = loss_fn(log_probs, y_batch)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.sum().item()\n",
        "\n",
        "  print(f\" Epoch: {epoch} Loss: {total_loss / len(examples)}\")\n",
        "\n",
        "  if total_loss / len(examples) < best_loss:\n",
        "    best_loss = total_loss / len(examples)\n",
        "    early_stop = 0\n",
        "\n",
        "  else:\n",
        "    eraly_stop += 1\n",
        "\n",
        "  if early_stop == 3:\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# vocab = ...\n",
        "# model = ...\n",
        "\n",
        "# ...\n",
        "\n",
        "############################################"
      ],
      "metadata": {
        "id": "tFb93XK8ErR7",
        "outputId": "ddcfed8c-2f32-4c09-d1f0-9d0c7b2f7112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 0 Loss: 0.0006280367939708394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 1 Loss: 0.0005592429849966121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 2 Loss: 0.0005381619467563101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 3 Loss: 0.0005244605601712597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 4 Loss: 0.0005142414947379261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 5 Loss: 0.0005046109520243945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 6 Loss: 0.0004965675858373678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 7 Loss: 0.000490119987528845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 8 Loss: 0.0004848899140141465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch: 9 Loss: 0.0004799686655750852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ß–∏—Ç–∞—î–º–æ –º—ñ–∂ —Ä—è–¥–∫—ñ–≤"
      ],
      "metadata": {
        "id": "USBCYglkn_WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import collections\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "# –î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω—Ü—ñ—è:\n",
        "def get_letter(w1: str, w2: str) -> str:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î –ª—ñ—Ç–µ—Ä—É, —è–∫–æ—ó –≤—ñ–¥—Ä—ñ–∑–Ω—è—é—Ç—å—Å—è —Å–ª–æ–≤–∞ –∞–±–æ –ø—Ä–æ–±—ñ–ª –¥–ª—è –æ–¥–Ω–∞–∫–æ–≤–∏—Ö —Å–ª—ñ–≤.\n",
        "    \"\"\"\n",
        "\n",
        "    letters1 = collections.Counter(w1)\n",
        "    letters2 = collections.Counter(w2)\n",
        "\n",
        "    diff = letters1 - letters2\n",
        "    if len(diff) != 1:\n",
        "        return \" \"\n",
        "\n",
        "    return diff.most_common()[0][0]\n",
        "\n",
        "\n",
        "def score_text(text: str, model, vocab) -> float:\n",
        "\n",
        "    tokens = tokenize(text)\n",
        "\n",
        "    total_log_prob = 0.0\n",
        "\n",
        "    for context, target in prepare_data(tokens, model.context_len):\n",
        "\n",
        "        X = vectorize(context, vocab)\n",
        "        target = vectorize([target], vocab)[0]\n",
        "        log_probs = model(X)\n",
        "        target_log_prob = log_probs[0, target]\n",
        "        total_log_prob += target_log_prob\n",
        "\n",
        "    return torch.exp(torch.tensor(-total_log_prob / len(tokens))).item()\n",
        "\n",
        "\n",
        "# –ú–æ–∂–µ—Ç–µ –∑–º—ñ–Ω—é–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç–∞ –≤–µ—Å—å —Ü–µ–π –∫–æ–¥, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n",
        "def solve(model, vocab, test_items) -> Tuple[List[str], str]:\n",
        "    \"\"\"–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑ —Ç–µ–∫—Å—Ç—ñ–≤ –≤ test_items —Ç–∞\n",
        "    —Å–µ–∫—Ä–µ—Ç–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.\n",
        "    \"\"\"\n",
        "\n",
        "    choices = []\n",
        "    secret = []\n",
        "\n",
        "    for item in test_items:\n",
        "        scores = []\n",
        "        for corr in item['corrections']:\n",
        "\n",
        "            # –ü—ñ–¥—Å—Ç–∞–≤–ª—è—î–º–æ —Å–ª–æ–≤–æ-–∫–∞–Ω–¥–∏–¥–∞—Ç –≤ —Ç–µ–∫—Å—Ç\n",
        "            text = item['text'][:item['error_start']] + corr + item['text'][item['error_end']:]\n",
        "\n",
        "            # –†–∞—Ö—É—î–º–æ score —Ç–µ–∫—Å—Ç—É\n",
        "            score = score_text(text, model, vocab)\n",
        "            scores.append(score)\n",
        "\n",
        "            # print(f'{score:.4f} {text}')\n",
        "\n",
        "        # –°–æ—Ä—Ç—É—î–º–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤ –Ω–∞ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∑–∞ score\n",
        "        result = sorted(zip(scores, item['corrections']), key=lambda x: x[0])\n",
        "\n",
        "        # –û–±–∏—Ä–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –∑–∞–º—ñ–Ω—É\n",
        "        best = result[0]\n",
        "        best_word = best[1]\n",
        "        choices.append(best_word)\n",
        "\n",
        "        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —á–µ—Ä–≥–æ–≤—É –ª—ñ—Ç–µ—Ä—É –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
        "        error = item['error']\n",
        "        letter = get_letter(error, best_word)\n",
        "        secret.append(letter)\n",
        "\n",
        "    secret_message = ''.join(secret)\n",
        "\n",
        "    return choices, secret_message\n",
        "\n",
        "choices, secret_message = solve(model, vocab, lab.test_items)\n",
        "\n",
        "lab.evaluate_accuracy(choices)\n",
        "print(\"SECRET MESSAGE: \", secret_message)\n"
      ],
      "metadata": {
        "id": "suXDePt3pLZ7",
        "outputId": "244f5ed5-e552-4585-85d8-955af7ed7fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-3fb6fd3ce320>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.exp(torch.tensor(-total_log_prob / len(tokens))).item()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "accuracy = 0.60. –í–∂–µ –∫—Ä–∞—â–µ. –ú–æ–∂–µ—à –ø–æ–ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞–¥ –º–æ–¥–µ–ª–ª—é —â–µ, –∞ –º–æ–∂–µ—à —Ä—É—Ö–∞—Ç–∏—Å—è –¥–∞–ª—ñ –π —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Ä–æ–∑–≥–∞–¥–∞—Ç–∏ –ø—Ä–∏—Ö–æ–≤–∞–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è\n",
            "SECRET MESSAGE:   –∫ –¥–æ–±—Ä–µ —Ç  —â–æ —Å–º–µ—Ä—Ç—ñ  –µ  –æ —Å—å—ñ—è —ñ –Ω  –ø–∏  —é —á   —è –∫–∏  –º –π —Ö—Ä —Å—Ç—Å—â–æ—á  –º –±–æ–ì–æ –µ   –∑ –∫–û–∞–Ω–µ–ø –ª–æ–Ω—é—Å—è –≤ –ø –†–µ–¥—á—É—Ç—Ç—ñ  –µ–¥ –≤—ñ–¥–æ–º–∏—Ö–Ω–≤ —Ä—Å—Ç–≤ —â–æ–∏ –∏ –ª—é–± –í   –Ω    –ë –∞–≤—Å –∑—Å–∫–≤–µ—Ä–Ω  –Ω –Ω–∞ –∏—Å   –ø–†–æ–∫–ª—å –Ω—É–≤–∫  —Ç—Ç –∏–Ω —Ä–æ–¥–µ   –π  –æ  –µ–±–µ —è —â  –≤–µ—Ä–Ω—É   –í —Å–º–µ–∑—Ç—ñ –æ–±–µ—Ä–Ω          —Ç—è —Å–≤ —ó  —Å—Ç—Ä –∂–¥ –Ω–Ω –º—è—ñ –Ω–µ–∑–ª –º –æ–±  —á—á—è–º    —Å–∏  —Ç–æ —ñ–ª–¥ –∑–µ  –æ –ø–æ–∫    —Å—å —ñ —á–µ–°   –≥ —è —É–ª–≤ —á–µ—Å–Ω—ñ  –≤–æ  –í—ñ   —ñ —á–µ—Å–Ω–∏–º–∏ —Å —å –∑     –± –ª–ª—é—Å—å\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.answer(\"–í–∞—Å–∏–ª—å –°—Ç—É—Å\")"
      ],
      "metadata": {
        "id": "S7Hmxf6YqUVR",
        "outputId": "1b6a2a70-6953-482d-f29b-139bbafb7b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø—Ä–∞–≤–∏–ª—å–Ω–∞ ‚úÖ\n",
            "–ü—Ä–∞–≤–∏–ª—å–Ω–æ! üöÄ –ó–∞–ø–æ–≤–Ω–∏ —Ç–µ–ø–µ—Ä —Ü—é —Ñ–æ—Ä–º—É, –±—É–¥—å –ª–∞—Å–∫–∞: https://tally.so/r/wkl0zZ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í—ñ–¥–ø—Ä–∞–≤—Ç–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Ü–µ–π colab –∞–±–æ PDF –∑ –Ω–∏–º –Ω–∞ –ø–æ—à—Ç—É oleksii.o.syvokon@lpnu.ua. –î—è–∫—É—é!\n"
      ],
      "metadata": {
        "id": "VpE_-wtdpkHe"
      }
    }
  ]
}